// Meyme - Modern AI Voice Agent JavaScript
// Modified to send 16kHz, 16-bit, mono PCM audio via Web Audio API

document.addEventListener('DOMContentLoaded', () => {
  const voiceButton = document.getElementById('voiceButton');
  const micIcon = document.getElementById('micIcon');
  const statusMessage = document.getElementById('statusMessage');
  let isRecording = false;
  let audioContext = null;
  let mediaStreamSource = null;
  let processor = null;
  let socket = null;
  let socketConnected = false;
  let accumulatedAudioChunks = []; // üéµ DAY 21: Array to accumulate base64 audio chunks
  
  // üéµ DAY 22: Audio playback management
  let playbackAudioContext = null;
  let audioQueue = [];
  let isPlayingAudio = false;
  let currentAudioSource = null;
  let playbackStartTime = 0;
  let totalPlaybackDuration = 0;

  const SAMPLE_RATE = 16000; // AssemblyAI required sample rate
  const BUFFER_SIZE = 4096; // Audio processing buffer size
  const PLAYBACK_SAMPLE_RATE = 44100; // Murf TTS audio sample rate

  const toggleRecording = async () => {
    if (isRecording) {
      stopRecording();
    } else {
      await startRecording();
    }
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      mediaStreamSource = audioContext.createMediaStreamSource(stream);

      // Create a ScriptProcessorNode to process audio samples
      // Deprecated, but widely supported. For modern apps, AudioWorkletNode is preferred.
      processor = audioContext.createScriptProcessor(BUFFER_SIZE, 1, 1); // bufferSize, inputChannels, outputChannels

      processor.onaudioprocess = (e) => {
        if (!isRecording || !socket || socket.readyState !== WebSocket.OPEN) {
          return;
        }

        // Get the audio data from the input buffer (first channel for mono)
        const inputData = e.inputBuffer.getChannelData(0);

        // Resample and convert to 16-bit PCM
        const downsampledBuffer = downsampleBuffer(inputData, audioContext.sampleRate, SAMPLE_RATE);
        const pcm16 = to16BitPCM(downsampledBuffer);

        // Send the 16-bit PCM data over the WebSocket
        socket.send(pcm16);
      };

      mediaStreamSource.connect(processor);
      processor.connect(audioContext.destination); // Connect to destination to keep the audio graph alive

      socket = new WebSocket(`ws://${window.location.host}/ws`);

      socket.onopen = () => {
        isRecording = true;
        updateUIForRecording();
      statusMessage.textContent = 'üéôÔ∏è Meyme is listening...';
        statusMessage.classList.add('show');
        
        // üéµ DAY 22: Stop any ongoing audio playback when starting to record
        if (isPlayingAudio) {
          stopAudioPlayback();
          console.log('üéµ üõë Stopped ongoing audio playback to start recording');
        }
        
        // üéµ DAY 21: Clear accumulated audio chunks for new session
        accumulatedAudioChunks = [];
        console.log('üîå ‚úÖ WebSocket connection established');
        console.log('üéµ ‚úÖ Ready for audio streaming');
        console.log('üßπ ‚úÖ Cleared accumulated audio chunks for new recording session');
        console.log('üéôÔ∏è ‚úÖ Microphone active and listening...');
      };

      socket.onmessage = async (event) => {
        try {
            const data = JSON.parse(event.data);
            console.log('Received websocket message:', data);

            switch(data.type) {
                case 'transcript':
                    if (data.end_of_turn) {
                        // Final transcript - add user message
                        appendMessage('user', data.transcript);
                        console.log('Added user message:', data.transcript);
                    } else if (data.is_partial) {
                        // Update interim message
                        appendMessage('interim', data.transcript);
                    }
                    break;

                case 'llm_response':
                    // Add AI response
                    appendMessage('assistant', data.text);
                    console.log('Added AI response:', data.text);
                    break;
            }
        } catch (e) {
            console.error('Error parsing WebSocket message:', e);
        }
    };

      socket.onclose = () => {
        console.log('üîå WebSocket connection closed');
        console.log('üõë Audio streaming session ended');
        stopRecording();
      };

      socket.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
        console.error('üõë Audio streaming interrupted');
        stopRecording();
      };

    } catch (error) {
      console.error('Error starting recording:', error);
      statusMessage.textContent = '‚ùå Microphone access denied. Please allow microphone permissions.';
    }
  };

  const stopRecording = () => {
    if (isRecording) {
      isRecording = false;
      
      // Signal end of turn to backend before closing
      if (socket && socket.readyState === WebSocket.OPEN) {
        try {
          socket.send(JSON.stringify({
            type: 'end_turn',
            message: 'User finished speaking'
          }));
          console.log('üéØ ‚úÖ Sent end_turn signal to backend');
        } catch (error) {
          console.error('‚ùå Error sending end_turn signal:', error);
        }
        
        // Keep the WebSocket open for audio streaming response
        // We'll close it after audio streaming completes or timeout
        console.log('üîå ‚ÑπÔ∏è WebSocket kept open for audio streaming response');
        
        // Set a timeout to close the connection if no audio response comes
        setTimeout(() => {
          if (socket && socket.readyState === WebSocket.OPEN) {
            console.log('‚è∞ üîå Closing WebSocket after timeout (no audio response)');
            socket.close();
          }
        }, 30000); // 30 second timeout for audio response
      }
      
      if (processor) {
        processor.disconnect();
        processor = null;
      }
      if (mediaStreamSource) {
        mediaStreamSource.disconnect();
        mediaStreamSource = null;
      }
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      
      updateUIForStopped();
      statusMessage.textContent = 'ü§ñ Meyme is thinking...';
      statusMessage.classList.remove('speaking', 'partial', 'turn-complete');
      statusMessage.classList.add('processing');
      
      console.log('üéØ ‚úÖ Recording stopped and processing initiated');
    }
  };

  // Function to downsample audio buffer
  function downsampleBuffer(buffer, originalSampleRate, newSampleRate) {
    if (newSampleRate === originalSampleRate) {
      return buffer;
    }
    const ratio = originalSampleRate / newSampleRate;
    const newLength = Math.round(buffer.length / ratio);
    const result = new Float32Array(newLength);
    let offsetResult = 0;
    let offsetBuffer = 0;
    while (offsetResult < result.length) {
      const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
      // Use average value for downsampling
      let accum = 0;
      let count = 0;
      for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
        accum += buffer[i];
        count++;
      }
      result[offsetResult] = accum / count;
      offsetResult++;
      offsetBuffer = nextOffsetBuffer;
    }
    return result;
  }

  // Function to convert Float32Array to 16-bit PCM (Int16Array)
  function to16BitPCM(input) {
    const dataLength = input.length * 2;
    const output = new Int16Array(dataLength / 2);
    for (let i = 0; i < input.length; i++) {
      let s = Math.max(-1, Math.min(1, input[i]));
      output[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return output.buffer; // Return as ArrayBuffer for WebSocket
  }

  const updateUIForRecording = () => {
    voiceButton.classList.add('recording');
    micIcon.className = 'fas fa-stop';
  };

  const updateUIForStopped = () => {
    voiceButton.classList.remove('recording');
    micIcon.className = 'fas fa-microphone';
  };

  // üéµ DAY 22: SEAMLESS AUDIO PLAYBACK FUNCTIONS
  
  // Initialize playback audio context
  async function initPlaybackAudioContext() {
    if (!playbackAudioContext) {
      try {
        playbackAudioContext = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: PLAYBACK_SAMPLE_RATE
        });
        
        // Resume context if it's suspended (required for some browsers)
        if (playbackAudioContext.state === 'suspended') {
          await playbackAudioContext.resume();
        }
        
        console.log('üéµ ‚úÖ Playback AudioContext initialized:', {
          sampleRate: playbackAudioContext.sampleRate,
          state: playbackAudioContext.state
        });
        
      } catch (error) {
        console.error('‚ùå Error initializing playback audio context:', error);
        throw error;
      }
    }
    return playbackAudioContext;
  }
  
  // Convert base64 audio to AudioBuffer
  async function base64ToAudioBuffer(base64Audio, chunkIndex) {
    try {
      // Decode base64 to ArrayBuffer
      const binaryString = atob(base64Audio);
      const bytes = new Uint8Array(binaryString.length);
      for (let i = 0; i < binaryString.length; i++) {
        bytes[i] = binaryString.charCodeAt(i);
      }
      
      // Initialize audio context if needed
      const context = await initPlaybackAudioContext();
      
      // For debugging, let's check if this looks like a valid WAV file
      const wavHeader = binaryString.substring(0, 12);
      const isValidWav = wavHeader.startsWith('RIFF') && wavHeader.includes('WAVE');
      
      if (!isValidWav && chunkIndex > 1) {
        // This chunk might be raw audio data, not a complete WAV file
        // For now, skip non-WAV chunks (most chunks after the first seem to be raw data)
        console.log(`üéµ ‚ö†Ô∏è Chunk #${chunkIndex} appears to be raw audio data, skipping playback`);
        throw new Error(`Chunk #${chunkIndex} is not a valid WAV file`);
      }
      
      // Decode audio data to AudioBuffer
      const audioBuffer = await context.decodeAudioData(bytes.buffer);
      
      console.log('üéµ ‚úÖ Audio buffer decoded:', {
        duration: audioBuffer.duration.toFixed(3) + 's',
        sampleRate: audioBuffer.sampleRate,
        channels: audioBuffer.numberOfChannels,
        length: audioBuffer.length,
        isValidWav: isValidWav
      });
      
      return audioBuffer;
      
    } catch (error) {
      console.error(`‚ùå Error converting base64 to AudioBuffer (chunk #${chunkIndex}):`, error);
      throw error;
    }
  }
  
  // Play a single audio chunk seamlessly
  async function playAudioChunk(base64Audio, chunkIndex) {
    try {
      console.log(`üéµ üöÄ PLAYING AUDIO CHUNK #${chunkIndex}`);
      console.log('üéµ' + '-'.repeat(40));
      
      // Convert base64 to audio buffer
      const audioBuffer = await base64ToAudioBuffer(base64Audio, chunkIndex);
      
      // Initialize audio context
      const context = await initPlaybackAudioContext();
      
      // Create audio source
      const source = context.createBufferSource();
      source.buffer = audioBuffer;
      
      // Connect to destination (speakers)
      source.connect(context.destination);
      
      // Calculate when to start playing this chunk
      let startTime = context.currentTime;
      
      if (isPlayingAudio) {
        // If already playing, schedule this chunk to play after the previous one
        startTime = Math.max(context.currentTime, playbackStartTime + totalPlaybackDuration);
      } else {
        // First chunk - start immediately
        isPlayingAudio = true;
        playbackStartTime = context.currentTime;
        totalPlaybackDuration = 0;
        
        console.log('üéµ ‚úÖ STARTING SEAMLESS AUDIO PLAYBACK PIPELINE');
      }
      
      // Update total duration
      totalPlaybackDuration += audioBuffer.duration;
      
      console.log(`üéµ üìÖ AUDIO TIMING:`);
      console.log(`   ‚è±Ô∏è  Start time: ${(startTime - context.currentTime).toFixed(3)}s from now`);
      console.log(`   ‚è±Ô∏è  Chunk duration: ${audioBuffer.duration.toFixed(3)}s`);
      console.log(`   ‚è±Ô∏è  Total pipeline duration: ${totalPlaybackDuration.toFixed(3)}s`);
      
      // Start playing at the calculated time
      source.start(startTime);
      
      // Handle playback completion
      source.onended = () => {
        console.log(`üéµ ‚úÖ CHUNK #${chunkIndex} PLAYBACK COMPLETED`);
        
        // Check if this is the last chunk playing
        if (context.currentTime >= playbackStartTime + totalPlaybackDuration - 0.1) {
          console.log('üéµ üéâ ALL AUDIO CHUNKS PLAYBACK COMPLETED!');
          isPlayingAudio = false;
          playbackStartTime = 0;
          totalPlaybackDuration = 0;
          
          // Update UI to show conversation is ready
          setTimeout(() => {
            statusMessage.textContent = 'üéôÔ∏è Press the mic button to speak';
            statusMessage.classList.remove('speaking', 'processing');
          }, 500);
        }
      };
      
      // Store reference to current source
      currentAudioSource = source;
      
      console.log(`üéµ ‚úÖ CHUNK #${chunkIndex} QUEUED FOR SEAMLESS PLAYBACK`);
      console.log('üéµ' + '-'.repeat(40));
      
    } catch (error) {
      console.error(`‚ùå Error playing audio chunk #${chunkIndex}:`, error);
      
      // Fallback - try to continue with next chunk
      console.log('‚ö†Ô∏è  Attempting to continue with next audio chunk...');
    }
  }
  
  // Stop current audio playback (useful for interruptions)
  function stopAudioPlayback() {
    if (currentAudioSource) {
      try {
        currentAudioSource.stop();
        currentAudioSource = null;
      } catch (error) {
        console.log('Audio source already stopped');
      }
    }
    
    isPlayingAudio = false;
    playbackStartTime = 0;
    totalPlaybackDuration = 0;
    audioQueue = [];
    
    console.log('üéµ üõë Audio playback stopped and reset');
  }
  
  // üéµ DAY 22: Assemble and play complete audio from all chunks (Murf-style)
  async function assembleAndPlayCompleteAudio(audioChunks) {
    try {
      console.log('üéµ üßÆ ASSEMBLING COMPLETE AUDIO FROM ALL CHUNKS (MURF STYLE)');
      console.log('üéµ' + '='.repeat(70));
      console.log(`üì¶ Total chunks to assemble: ${audioChunks.length}`);
      
      if (audioChunks.length === 0) {
        throw new Error('No audio chunks to assemble');
      }
      
      // Stop any ongoing playback first
      if (isPlayingAudio) {
        stopAudioPlayback();
      }
      
      // Use Murf's recommended approach: combine WAV chunks
      await playCombinedWavChunks(audioChunks);
      
    } catch (error) {
      console.error('‚ùå Error assembling complete audio:', error);
      statusMessage.textContent = '‚ùå Audio assembly failed - trying fallback...';
      
      // Final fallback: try to play just the first chunk
      try {
        if (audioChunks.length > 0) {
          const fallbackBuffer = await base64ToAudioBuffer(audioChunks[0], 'EMERGENCY_FALLBACK');
          await playAssembledAudio(fallbackBuffer);
        }
      } catch (fallbackError) {
        console.error('‚ùå Even fallback failed:', fallbackError);
        statusMessage.textContent = '‚ùå Audio playback failed completely';
        
        // Reset state on complete failure
        isPlayingAudio = false;
        currentAudioSource = null;
        
        setTimeout(() => {
          statusMessage.textContent = 'üé¨ Press the mic button to try again';
        }, 3000);
      }
    }
  }
  
  // üéµ DAY 22: Murf-style WAV chunk combination and playback
  async function playCombinedWavChunks(base64Chunks) {
    try {
      console.log('üéµ üî® COMBINING WAV CHUNKS (MURF COOKBOOK APPROACH)');
      console.log('üéµ' + '='.repeat(60));
      
      const pcmData = [];
      const SAMPLE_RATE = 44100;
      const NUM_CHANNELS = 1;
      const BIT_DEPTH = 16;
      
      // Process each chunk according to Murf's specification
      for (let i = 0; i < base64Chunks.length; i++) {
        console.log(`  üîß Processing chunk ${i + 1}/${base64Chunks.length}`);
        
        try {
          const bytes = base64ToUint8Array(base64Chunks[i]);
          
          if (i === 0) {
            // First chunk: complete WAV file, skip 44-byte header to get PCM data
            console.log(`    üìã First chunk: complete WAV file`);
            console.log(`    üìè Original length: ${bytes.length} bytes`);
            
            // Verify it's a valid WAV
            const wavHeader = String.fromCharCode(...bytes.slice(0, 12));
            if (wavHeader.startsWith('RIFF') && wavHeader.includes('WAVE')) {
              const pcmPortion = bytes.slice(44); // Skip 44-byte WAV header
              console.log(`    üéµ PCM data extracted: ${pcmPortion.length} bytes`);
              pcmData.push(pcmPortion);
            } else {
              console.warn(`    ‚ö†Ô∏è  First chunk doesn't appear to be valid WAV, using as-is`);
              pcmData.push(bytes);
            }
          } else {
            // Subsequent chunks: should be raw PCM data
            console.log(`    üéµ Chunk ${i + 1}: raw PCM data (${bytes.length} bytes)`);
            pcmData.push(bytes);
          }
        } catch (chunkError) {
          console.warn(`    ‚ö†Ô∏è  Error processing chunk ${i + 1}:`, chunkError);
          // Skip problematic chunks and continue
          continue;
        }
      }
      
      // Combine all PCM chunks
      const totalPcmLength = pcmData.reduce((sum, chunk) => sum + chunk.length, 0);
      console.log(`üìä Total PCM data length: ${totalPcmLength.toLocaleString()} bytes`);
      
      const combinedPcm = new Uint8Array(totalPcmLength);
      let offset = 0;
      
      for (const chunk of pcmData) {
        combinedPcm.set(chunk, offset);
        offset += chunk.length;
      }
      
      console.log(`‚úÖ Combined ${pcmData.length} chunks into ${combinedPcm.length.toLocaleString()} bytes of PCM`);
      
      // Create new WAV header for the combined PCM data
      const wavHeader = createWavHeader(combinedPcm.length, SAMPLE_RATE, NUM_CHANNELS, BIT_DEPTH);
      console.log(`üìã Created new WAV header (${wavHeader.length} bytes)`);
      
      // Combine header + PCM data
      const finalWav = new Uint8Array(wavHeader.length + combinedPcm.length);
      finalWav.set(wavHeader, 0);
      finalWav.set(combinedPcm, wavHeader.length);
      
      console.log(`üéµ Final WAV file: ${finalWav.length.toLocaleString()} bytes total`);
      console.log(`‚è±Ô∏è  Estimated duration: ${(combinedPcm.length / (SAMPLE_RATE * NUM_CHANNELS * (BIT_DEPTH / 8))).toFixed(2)} seconds`);
      
      // Create blob and object URL
      const blob = new Blob([finalWav], { type: 'audio/wav' });
      const audioUrl = URL.createObjectURL(blob);
      
      console.log('üéµ' + '='.repeat(60));
      console.log('üéµ ‚ñ∂Ô∏è STARTING MURF-STYLE COMBINED AUDIO PLAYBACK');
      
      // Get the audio element and set up playback
      const audioPlayer = document.getElementById('meymeAudioPlayer');
      if (!audioPlayer) {
        throw new Error('Audio player element not found');
      }
      
      // Set up the audio player
      audioPlayer.src = audioUrl;
      audioPlayer.style.display = 'none'; // Hide the actual player element
      
      // Update status
      statusMessage.textContent = `üéµ Meyme is speaking...`;
      statusMessage.classList.remove('processing', 'turn-complete', 'partial');
      statusMessage.classList.add('speaking');
      
      // Set up event listeners
      const onLoadedData = () => {
        console.log(`üéµ ‚úÖ Audio loaded successfully, duration: ${audioPlayer.duration.toFixed(2)}s`);
        
        // Set playback state
        isPlayingAudio = true;
        
        // Auto-play the audio
        audioPlayer.play().then(() => {
          console.log('üéµ ‚ñ∂Ô∏è MURF-STYLE COMBINED AUDIO PLAYBACK STARTED!');
        }).catch(autoPlayError => {
          console.log('‚ö†Ô∏è  Auto-play blocked, user can manually play:', autoPlayError);
          statusMessage.textContent = 'üéµ Click the audio player to hear Meyme\'s complete response';
        });
      };
      
      const onEnded = () => {
        console.log('üéµ üéâ MURF-STYLE COMBINED AUDIO PLAYBACK FINISHED!');
        
        // Clean up
        isPlayingAudio = false;
        URL.revokeObjectURL(audioUrl);
        audioPlayer.removeEventListener('loadeddata', onLoadedData);
        audioPlayer.removeEventListener('ended', onEnded);
        audioPlayer.removeEventListener('error', onError);
        
        // Update UI to show conversation is complete
        setTimeout(() => {
          accumulatedAudioChunks = [];
          statusMessage.textContent = 'üéôÔ∏è Press the mic button to speak';
          statusMessage.classList.remove('speaking', 'processing');
          console.log('üßπ ‚úÖ Ready for new conversation');
        }, 500);
      };
      
      const onError = (error) => {
        console.error('‚ùå Audio playback error:', error);
        statusMessage.textContent = '‚ùå Audio playback failed';
        statusMessage.classList.remove('speaking', 'processing');
        
        // Clean up
        isPlayingAudio = false;
        audioPlayer.classList.remove('show');
        URL.revokeObjectURL(audioUrl);
        audioPlayer.removeEventListener('loadeddata', onLoadedData);
        audioPlayer.removeEventListener('ended', onEnded);
        audioPlayer.removeEventListener('error', onError);
      };
      
      // Add event listeners
      audioPlayer.addEventListener('loadeddata', onLoadedData);
      audioPlayer.addEventListener('ended', onEnded);
      audioPlayer.addEventListener('error', onError);
      
      console.log('üéµ ‚è≥ Loading combined WAV into audio player...');
      
    } catch (error) {
      console.error('‚ùå Error in playCombinedWavChunks:', error);
      throw error;
    }
  }
  
  // üéµ Helper function to play assembled audio buffer
  async function playAssembledAudio(audioBuffer) {
    try {
      const context = await initPlaybackAudioContext();
      
      // Create audio source
      const source = context.createBufferSource();
      source.buffer = audioBuffer;
      
      // Connect to destination (speakers)
      source.connect(context.destination);
      
      // Set playback state
      isPlayingAudio = true;
      playbackStartTime = context.currentTime;
      totalPlaybackDuration = audioBuffer.duration;
      currentAudioSource = source;
      
      console.log('üéµ üé§ FINAL ASSEMBLED AUDIO PLAYBACK INFO:');
      console.log(`   ‚è±Ô∏è  Total duration: ${audioBuffer.duration.toFixed(3)} seconds`);
      console.log(`   üîâ Sample rate: ${audioBuffer.sampleRate} Hz`);
      console.log(`   üéß Channels: ${audioBuffer.numberOfChannels}`);
      console.log(`   üìè Audio samples: ${audioBuffer.length.toLocaleString()}`);
      console.log('üéµ' + '='.repeat(60));
      
      // Update status
      statusMessage.textContent = `üéµ Meyme is speaking...`;
      
      // Start playing immediately
      source.start(0);
      console.log('üéµ ‚ñ∂Ô∏è FINAL ASSEMBLED AUDIO PLAYBACK STARTED!');
      
      // Handle playback completion
      source.onended = () => {
        console.log('üéµ üéâ COMPLETE ASSEMBLED AUDIO PLAYBACK FINISHED!');
        
        isPlayingAudio = false;
        playbackStartTime = 0;
        totalPlaybackDuration = 0;
        currentAudioSource = null;
        
        // Update UI to show conversation is complete and ready for next input
        setTimeout(() => {
          // Clear accumulated chunks for next conversation
          accumulatedAudioChunks = [];
          statusMessage.textContent = 'üéôÔ∏è Press the mic button to speak';
          statusMessage.classList.remove('speaking', 'processing');
          console.log('üßπ ‚úÖ Cleared accumulated audio chunks for next conversation');
          console.log('üßπ ‚úÖ Ready for new audio streaming session');
        }, 500);
      };
      
    } catch (error) {
      console.error('‚ùå Error in playAssembledAudio:', error);
      throw error;
    }
  }
  
  // üéµ DAY 22: Helper function to convert base64 to Uint8Array
  function base64ToUint8Array(base64) {
    const binary = atob(base64);
    const len = binary.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
      bytes[i] = binary.charCodeAt(i);
    }
    return bytes;
  }
  
  // üéµ DAY 22: Helper function to create WAV header
  function createWavHeader(dataLength, sampleRate = 44100, numChannels = 1, bitDepth = 16) {
    const blockAlign = (numChannels * bitDepth) / 8;
    const byteRate = sampleRate * blockAlign;
    const buffer = new ArrayBuffer(44);
    const view = new DataView(buffer);
    
    function writeStr(offset, str) {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    }
    
    // RIFF chunk descriptor
    writeStr(0, 'RIFF');
    view.setUint32(4, 36 + dataLength, true); // file size - 8
    writeStr(8, 'WAVE');
    
    // fmt sub-chunk
    writeStr(12, 'fmt ');
    view.setUint32(16, 16, true); // sub-chunk size (16 for PCM)
    view.setUint16(20, 1, true); // audio format (1 = PCM)
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, byteRate, true);
    view.setUint16(32, blockAlign, true);
    view.setUint16(34, bitDepth, true);
    
    // data sub-chunk
    writeStr(36, 'data');
    view.setUint32(40, dataLength, true);
    
    return new Uint8Array(buffer);
  }
  
  // üéµ DAY 22: Legacy function (kept for compatibility)
  async function playCompleteAudio(base64Audio) {
    console.log('‚ö†Ô∏è  Using legacy playCompleteAudio - consider using assembleAndPlayCompleteAudio instead');
    const audioBuffer = await base64ToAudioBuffer(base64Audio, 'LEGACY');
    await playAssembledAudio(audioBuffer);
  }

  voiceButton.addEventListener('click', toggleRecording);

  function appendMessage(type, text) {
    const chatHistory = document.getElementById('chatHistory');
    if (!chatHistory) {
        console.error('Chat history element not found');
        return;
    }

    // Remove existing interim message if adding a new message
    if (type !== 'interim') {
        const existingInterim = document.getElementById('interimMessage');
        if (existingInterim) {
            existingInterim.remove();
        }
    }

    const messageDiv = document.createElement('div');
    messageDiv.className = `message ${type}-message`;
    
    if (type === 'interim') {
        messageDiv.id = 'interimMessage';
    }
    
    messageDiv.textContent = text;
    chatHistory.appendChild(messageDiv);
    chatHistory.scrollTop = chatHistory.scrollHeight;
}
});